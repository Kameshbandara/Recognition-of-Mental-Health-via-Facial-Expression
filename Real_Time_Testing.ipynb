{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image captured and saved as Captured_Images\\captured_20241206_103618.jpg\n",
      "Image captured and saved as Captured_Images\\captured_20241206_103622.jpg\n",
      "Image captured and saved as Captured_Images\\captured_20241206_103626.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Dictionary of selected facial landmark indices\n",
    "selected_indices = {\n",
    "    \"Eyebrows\": [21, 22, 23, 24],\n",
    "    \"Eyes\": [36, 39, 41, 42, 45, 47],\n",
    "    \"Nose\": [30, 31, 35],\n",
    "    \"Mouth\": [48, 51, 54, 55, 61],\n",
    "    \"Jawline\": [1, 9]\n",
    "}\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Initialize OpenCV Video Capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create a folder to save captured images if not exists\n",
    "save_folder = \"Captured_Images\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Flip the frame horizontally for better viewing\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Convert the frame to RGB for face mesh processing\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the frame to detect face landmarks\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for category, indices in selected_indices.items():\n",
    "                for idx in indices:\n",
    "                    # Get the coordinates of the landmark\n",
    "                    h, w, _ = frame.shape\n",
    "                    x = int(face_landmarks.landmark[idx].x * w)\n",
    "                    y = int(face_landmarks.landmark[idx].y * h)\n",
    "                    \n",
    "                    # Draw a circle around the selected landmarks\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "    \n",
    "    # Display the frame with the landmarks\n",
    "    cv2.imshow(\"Live Face Landmarks\", frame)\n",
    "    \n",
    "    # Capture the image if 'c' is pressed\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('c'):\n",
    "        # Save the current frame with a timestamped filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = os.path.join(save_folder, f\"captured_{timestamp}.jpg\")\n",
    "        cv2.imwrite(filename, frame)\n",
    "        print(f\"Image captured and saved as {filename}\")\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 11 features, but RandomForestClassifier is expecting 4 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform([features])\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Predict the stress level using the trained model\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Draw landmarks on the frame\u001b[39;00m\n\u001b[0;32m    121\u001b[0m draw_landmarks(frame, face_landmarks)\n",
      "File \u001b[1;32mc:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:820\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    800\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    823\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:862\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    860\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    861\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    865\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:602\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 602\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 11 features, but RandomForestClassifier is expecting 4 features as input."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import joblib\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model = joblib.load('stress_detection_model_distance_05.pkl')\n",
    "scaler = joblib.load('scaler_05.pkl')\n",
    "\n",
    "# Initialize Mediapipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Define landmarks to extract\n",
    "selected_indices = {\n",
    "    \"Eyebrows\": [21, 22, 23, 24],\n",
    "    \"Eyes\": [36, 39, 41, 42, 45, 47],\n",
    "    \"Nose\": [30, 31, 35],\n",
    "    \"Mouth\": [48, 51, 54, 55, 61],\n",
    "    \"Jawline\": [1, 9]\n",
    "}\n",
    "all_indices = [idx for indices in selected_indices.values() for idx in indices]\n",
    "\n",
    "# Define pairs of landmarks for distance calculation\n",
    "distance_pairs = [\n",
    "    (21, 22),  # Eyebrow width\n",
    "    (36, 39),  # Left eye width\n",
    "    (42, 45),  # Right eye width\n",
    "    (30, 48),  # Nose to mouth corner (left)\n",
    "    (30, 54),  # Nose to mouth corner (right)\n",
    "    (48, 54),  # Mouth width\n",
    "    (1, 9),    # Jawline height\n",
    "    (51, 61),  # Vertical mouth opening\n",
    "    (21, 23)   # Vertical eyebrow distance\n",
    "]\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define landmarks drawing function\n",
    "def draw_landmarks(frame, face_landmarks):\n",
    "    for feature, indices in selected_indices.items():\n",
    "        for i in indices:\n",
    "            if face_landmarks.landmark[i].visibility > 0:  # Ensure the landmark is visible\n",
    "                # Draw the landmark with blue color for specific facial features\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, face_landmarks, [i],\n",
    "                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),  \n",
    "                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)   \n",
    "                )\n",
    "\n",
    "    # Draw all contours with the original color scheme\n",
    "    mp_drawing.draw_landmarks(\n",
    "        frame, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        mp_drawing.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1),\n",
    "        mp_drawing.DrawingSpec(color=(80, 256, 121), thickness=1, circle_radius=1)\n",
    "    )\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a more natural selfie view\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with FaceMesh\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Extract 2D landmarks\n",
    "            landmark_coords = []\n",
    "            for idx in all_indices:\n",
    "                x = face_landmarks.landmark[idx].x\n",
    "                y = face_landmarks.landmark[idx].y\n",
    "                landmark_coords.append((x, y))\n",
    "\n",
    "            # Calculate inter-pupillary distance for normalization\n",
    "            left_eye = landmark_coords[all_indices.index(36)]\n",
    "            right_eye = landmark_coords[all_indices.index(42)]\n",
    "            inter_pupillary_dist = distance.euclidean(left_eye, right_eye)\n",
    "\n",
    "            # Calculate distances\n",
    "            distances = []\n",
    "            for pair in distance_pairs:\n",
    "                point1 = landmark_coords[all_indices.index(pair[0])]\n",
    "                point2 = landmark_coords[all_indices.index(pair[1])]\n",
    "                dist = distance.euclidean(point1, point2)\n",
    "                normalized_dist = dist / inter_pupillary_dist  # Normalize distance\n",
    "                distances.append(normalized_dist)\n",
    "\n",
    "            # Calculate additional ratios as features\n",
    "            mouth_width = distances[distance_pairs.index((48, 54))]\n",
    "            mouth_height = distances[distance_pairs.index((51, 61))]\n",
    "            if mouth_height > 0:  # Avoid division by zero\n",
    "                mouth_ratio = mouth_width / mouth_height\n",
    "            else:\n",
    "                mouth_ratio = 0\n",
    "\n",
    "            eyebrow_width = distances[distance_pairs.index((21, 22))]\n",
    "            eyebrow_height = distances[distance_pairs.index((21, 23))]\n",
    "            if eyebrow_height > 0:\n",
    "                eyebrow_ratio = eyebrow_width / eyebrow_height\n",
    "            else:\n",
    "                eyebrow_ratio = 0\n",
    "\n",
    "            # Append ratios and distances for prediction\n",
    "            features = distances + [mouth_ratio, eyebrow_ratio]\n",
    "\n",
    "            # Scale the features before prediction\n",
    "            features_scaled = scaler.transform([features])\n",
    "\n",
    "            # Predict the stress level using the trained model\n",
    "            prediction = model.predict(features_scaled)\n",
    "\n",
    "            # Draw landmarks on the frame\n",
    "            draw_landmarks(frame, face_landmarks)\n",
    "\n",
    "            # Display the prediction\n",
    "            stress_label = \"Stress\" if prediction == 1 else \"Non-Stress\"\n",
    "            cv2.putText(frame, f\"Prediction: {stress_label}\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    else:\n",
    "        # If no face detected, display message\n",
    "        cv2.putText(frame, \"No Face Detected\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the frame with landmarks and prediction\n",
    "    cv2.imshow('Face Stress Detection', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
